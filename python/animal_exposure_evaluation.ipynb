{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6c9f13-3a0f-42c3-9d52-28cb1d3a1b05",
   "metadata": {},
   "source": [
    "# This notebook provides example code for how a model was trained to identify potential Animal Exposures (and Other Pathogen Exposures) using SetFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f0d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import socket\n",
    "import operator\n",
    "import glob\n",
    "import collections\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer, sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b864f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our custom code\n",
    "sys.path.insert(0, r\"..\")\n",
    "\n",
    "# get our custom code\n",
    "sys.path.insert(0, r\"../data\")\n",
    "\n",
    "from animal_exposure_sentences import *\n",
    "from other_exposure_sentences import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "227845a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/paraphrase-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "sentence_model_dir = \"\"\"sentence-transformers/paraphrase-mpnet-base-v2\"\"\"\n",
    "    \n",
    "print(sentence_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9fd2d",
   "metadata": {},
   "source": [
    "# This is now sample code from the SetFit repo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da441a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "affirmed_examples = []\n",
    "negated_examples = []\n",
    "no_exposure_examples = []\n",
    "\n",
    "# first let's seet up a dataset of different classes of data\n",
    "    # just to recall, in other annotaiton, the convention has been:\n",
    "    # 0 - negated (denied) exposure\n",
    "    # 1 - affirmed exposure\n",
    "    # 2 - no mention of exposure\n",
    "\n",
    "label_names = ['DENIED_OR_NEGATED', 'AFFIRMED', 'NO_MENTION']\n",
    "    \n",
    "affirmed_examples = []\n",
    "negated_examples = []\n",
    "no_exposure_examples = []\n",
    "\n",
    "affirmed_examples = affirmed_animal_examples_masked\n",
    "negated_examples = negated_animal_examples_masked\n",
    "no_exposure_examples = no_exposure_animal_examples_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2643f5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(affirmed_examples): 5\n",
      "len(negated_examples): 5\n",
      "len(no_exposure_examples): 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(affirmed_examples): {len(affirmed_examples)}\")\n",
    "print(f\"len(negated_examples): {len(negated_examples)}\")\n",
    "print(f\"len(no_exposure_examples): {len(no_exposure_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17260020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# now let's prepare some labels\n",
    "affirmed_example_labels = [1] * len(affirmed_examples)\n",
    "negated_example_labels = [0] * len(negated_examples)\n",
    "no_exposure_example_labels = [2] * len(no_exposure_examples)\n",
    "\n",
    "# let's put these together in the same order to keep associations with labels:\n",
    "X = affirmed_examples + negated_examples + no_exposure_examples\n",
    "y = affirmed_example_labels + negated_example_labels + no_exposure_example_labels\n",
    "\n",
    "print(affirmed_example_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325ca989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now a data split using scikit-learn ...\n",
    "\n",
    "# In the paper, less training data was used to \n",
    "TRAINING_DATA_SPLIT = 0.8\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, stratify = y, \n",
    "                                                                train_size = TRAINING_DATA_SPLIT,\n",
    "                                                               test_size = 1.0 - TRAINING_DATA_SPLIT,\n",
    "                                                               random_state = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "486a5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now turn this into a HuggingFace dataset...\n",
    "dataset_dict_train = {'text': X_train,\n",
    "                     'label': y_train}\n",
    "\n",
    "dataset_dict_validation = {'text': X_validation,\n",
    "                     'label': y_validation}\n",
    "\n",
    "train_dataset = Dataset.from_dict(dataset_dict_train)\n",
    "eval_dataset = Dataset.from_dict(dataset_dict_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21dc281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'evaluate_modules.metrics.evaluate-metric--accuracy.f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14.accuracy.Accuracy'>\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "print(type(accuracy_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fc9bc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "C:\\Users\\slick\\AppData\\Local\\Temp\\ipykernel_6908\\2551628917.py:23: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2808b91ad5764a10aff0795417576c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a SetFit model from local dir\n",
    "model = SetFitModel.from_pretrained(sentence_model_dir,\n",
    "                                   local_files_only = True)\n",
    "\n",
    "# let's take a look at the base of this model and its tokens\n",
    "# learned how to get this from the Sentence Transformer (SBERT) documentation on adding special tokens\n",
    "model_body = model.model_body\n",
    "\n",
    "word_embedding_model = model_body._first_module()\n",
    "tokenizer = word_embedding_model.tokenizer\n",
    "\n",
    "# Small number for example purposes, but this is higher in the paper:\n",
    "NUM_EPOCHS = 1\n",
    "#NUM_EPOCHS = 4\n",
    "\n",
    "# Small number for example purposes, but this is higher in the paper:\n",
    "NUM_ITERATIONS = 1\n",
    "#NUM_ITERATIONS = 4\n",
    "\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    #batch_size=32,\n",
    "    batch_size=16,\n",
    "    num_iterations=NUM_ITERATIONS, # The number of text pairs to generate for contrastive learning\n",
    "    num_epochs=NUM_EPOCHS, # The number of epochs to use for contrastive learning\n",
    "    column_mapping={\"text\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "480232d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 24\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b888b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70537978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation model inference time: [0.2] seconds\n"
     ]
    }
   ],
   "source": [
    "# let's also get a classification report for this few shot model on the validation set\n",
    "\n",
    "# Now we can run our model and get predictions for these classes\n",
    "start_time = time.time()\n",
    "\n",
    "validation_pred = model(X_validation)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total validation model inference time: [{end_time - start_time:.1f}] seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15cb5ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of few-shot model on Validation Set\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "DENIED_OR_NEGATED      0.000     0.000     0.000         1\n",
      "         AFFIRMED      0.500     1.000     0.667         1\n",
      "       NO_MENTION      1.000     1.000     1.000         1\n",
      "\n",
      "         accuracy                          0.667         3\n",
      "        macro avg      0.500     0.667     0.556         3\n",
      "     weighted avg      0.500     0.667     0.556         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slick\\.conda\\envs\\emerging_public_health_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\slick\\.conda\\envs\\emerging_public_health_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\slick\\.conda\\envs\\emerging_public_health_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print('Validation of few-shot model on Validation Set')\n",
    "print(classification_report(y_validation, \n",
    "                            validation_pred,\n",
    "                           target_names = label_names,\n",
    "                            digits = 3\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c9f5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3])\n",
      "Prediction [1] for text: [Patient has been in contact with wild animals]\n",
      "Prediction [0] for text: [Patient denies any contact with wild animals]\n",
      "Prediction [1] for text: [Patient worked on amphibian vehicles during his service]\n"
     ]
    }
   ],
   "source": [
    "# finally let's try to make a prediction on some very different data:\n",
    "\n",
    "experiment_texts = [\"Patient has been in contact with wild animals\", \n",
    "               \"Patient denies any contact with wild animals\", \n",
    "               \"Patient worked on amphibian vehicles during his service\"]\n",
    "\n",
    "preds = model(experiment_texts)\n",
    "\n",
    "print(type(preds))\n",
    "\n",
    "print(preds.shape)\n",
    "\n",
    "for idx in range(len(experiment_texts)):\n",
    "    pred = preds[idx]\n",
    "    \n",
    "    print(f'Prediction [{pred.item()}] for text: [{experiment_texts[idx]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9125bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-emerging_public_health_nlp] *",
   "language": "python",
   "name": "conda-env-.conda-emerging_public_health_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
